{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, set_seed\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertLMHeadModel\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import pipeline, DataCollatorForLanguageModeling\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file_path):\n",
    "    final_path = \"standardized_text/\" + file_path\n",
    "    sentences = []\n",
    "\n",
    "    with open(final_path,\"r\") as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First:\n",
    "We first grab the codeswitched sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanglish_sentences = get_sentences(\"spanglish/sentimix2020.out\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then declare the translation models... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/Desktop/Self-Vs-Outside-Priming/.venv/lib/python3.7/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "es_en_translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "en_es_translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass the sentences through the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_sentences = [es_en_translator(sentence) for sentence in spanglish_sentences]\n",
    "en_es_sentences = [en_es_translator(sentence) for sentence in spanglish_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_es_en_sentences = []\n",
    "flattened_en_es_sentences = []\n",
    "\n",
    "for sentence in es_en_sentences:\n",
    "    flattened_es_en_sentences.append(sentence[0]['translation_text'])\n",
    "\n",
    "for sentence in en_es_sentences:\n",
    "    flattened_en_es_sentences.append(sentence[0]['translation_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"standardized_text/spanglish_translated_en_es.out\", \"w\") as f:\n",
    "    for sentence in flattened_en_es_sentences:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "with open(\"standardized_text/spanglish_translated_es_en.out\", \"w\") as f:\n",
    "    for sentence in flattened_es_en_sentences:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_es_en_sentences = []\n",
    "flattened_en_es_sentences = []\n",
    "\n",
    "with open(\"standardized_text/spanglish_translated_en_es.out\", \"r\") as f:\n",
    "    tmp = []\n",
    "    for sentence in f.readlines():\n",
    "        tmp.append(sentence[:-1])\n",
    "\n",
    "    flattened_en_es_sentences = tmp\n",
    "\n",
    "with open(\"standardized_text/spanglish_translated_es_en.out\", \"r\") as f:\n",
    "    tmp = []\n",
    "    for sentence in f.readlines():\n",
    "        tmp.append(sentence[:-1])\n",
    "\n",
    "    flattened_es_en_sentences = tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After translation is done:\n",
    "We load the model we wish to use into the notebook... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_model_id = \"gpt2-large\"\n",
    "eng_model = GPT2LMHeadModel.from_pretrained(eng_model_id)\n",
    "eng_tokenizer = GPT2TokenizerFast.from_pretrained(eng_model_id)\n",
    "eng_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "bert_eng_model_id = \"bert-base-uncased\"\n",
    "bert_eng_model = BertLMHeadModel.from_pretrained(bert_eng_model_id)\n",
    "bert_eng_tokenizer = BertTokenizer.from_pretrained(bert_eng_model_id)\n",
    "bert_eng_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 650/650 [00:00<00:00, 238kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:08<00:00, 53.5MB/s] \n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 14.9MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 134/134 [00:00<00:00, 246kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 310/310 [00:00<00:00, 618kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_model_id = \"flax-community/gpt-2-spanish\"\n",
    "span_model = GPT2LMHeadModel.from_pretrained(span_model_id)\n",
    "span_tokenizer = GPT2TokenizerFast.from_pretrained(span_model_id)\n",
    "span_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "bert_span_model_id = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "bert_span_model = BertLMHeadModel.from_pretrained(bert_span_model_id)\n",
    "bert_span_tokenizer = BertTokenizer.from_pretrained(bert_span_model_id)\n",
    "bert_span_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get the text we preprocessed earlier to get perplexity scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = get_sentences(\"eng/sentiment140_converted.out\")\n",
    "span_sentences = get_sentences(\"span/combined_tass2020_converted.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582575\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run it through the model's tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = eng_sentences[:1500]\n",
    "flattened_en_es_sentences = flattened_en_es_sentences[:1500]\n",
    "span_sentences = span_sentences[:1500]\n",
    "flattened_es_en_sentences = flattened_es_en_sentences[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(span_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_encodings = eng_tokenizer(eng_sentences, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "es_en_encodings = eng_tokenizer(flattened_es_en_sentences, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "span_encodings = span_tokenizer(span_sentences, padding='max_length', truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "en_es_encodings = span_tokenizer(flattened_en_es_sentences, padding='max_length', truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# print(encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_eng_encodings = bert_eng_tokenizer(eng_sentences, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "bert_es_en_encodings = bert_eng_tokenizer(flattened_es_en_sentences, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "bert_span_encodings = bert_span_tokenizer(span_sentences, padding='max_length', truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "bert_en_es_encodings = bert_span_tokenizer(flattened_en_es_sentences, padding='max_length', truncation=True, max_length=1024, return_tensors=\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After processing the data:\n",
    "We now can run it through the model to generate perplexity scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, sentences, tokenizer):\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = 1024\n",
    "    \n",
    "    ppls = []\n",
    "    for idx in tqdm(range(len(sentences))):\n",
    "        inputs = tokenizer(sentences[idx], truncation=True, padding=True, max_length=1024, return_tensors=\"pt\")\n",
    "        nlls = []\n",
    "        prev_end_loc = 0\n",
    "        for begin_loc in range(0, seq_len, stride):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "            input_ids = inputs.input_ids[:, begin_loc:end_loc]\n",
    "            #print(input_ids.shape)\n",
    "            target_ids = input_ids.clone()\n",
    "            #print(target_ids.shape)\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "                # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "                # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "                # to the left by 1.\n",
    "                neg_log_likelihood = outputs.loss\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "\n",
    "        ppls.append(torch.exp(torch.stack(nlls).mean()).item())\n",
    "    return statistics.mean(ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_file(sentences, dest_file):\n",
    "    with open(dest_file, \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_to_file(eng_sentences, \"standardized_text/eng/sentences.out\")\n",
    "sentences_to_file(span_sentences, \"standardized_text/span/sentences.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [22:43<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "eng_ppl = perplexity(eng_model, eng_sentences, eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [04:21<00:00,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "span_ppl = perplexity(span_model, span_sentences, span_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [22:40<00:00,  1.10it/s] \n"
     ]
    }
   ],
   "source": [
    "es_en_ppl = perplexity(eng_model, flattened_es_en_sentences, eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [04:04<00:00,  6.12it/s]\n"
     ]
    }
   ],
   "source": [
    "en_es_ppl = perplexity(span_model, flattened_en_es_sentences, span_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [04:03<00:00,  6.16it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_eng_ppl = perplexity(bert_eng_model, eng_sentences, bert_eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [04:11<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_span_ppl = perplexity(bert_span_model, span_sentences, bert_span_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [04:12<00:00,  5.94it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_es_en_ppl = perplexity(bert_eng_model, flattened_es_en_sentences, bert_eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [04:10<00:00,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_en_es_ppl = perplexity(bert_span_model, flattened_en_es_sentences, bert_span_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the mean perplexity over all the sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8324137.729295817\n",
      "1437454.7061899414\n",
      "11999733.95928854\n",
      "3954206.894305606\n"
     ]
    }
   ],
   "source": [
    "print(bert_eng_ppl)\n",
    "print(bert_span_ppl)\n",
    "print(bert_es_en_ppl)\n",
    "print(bert_en_es_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_spanglish_ppl = (es_en_ppl + en_es_ppl)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259.56097490469614\n",
      "160.0516792122523\n",
      "3319.269275833686\n",
      "1554.115388373693\n",
      "2436.6923321036893\n"
     ]
    }
   ],
   "source": [
    "print(eng_ppl)\n",
    "print(span_ppl)\n",
    "print(es_en_ppl)\n",
    "print(en_es_ppl)\n",
    "print(avg_spanglish_ppl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do these same steps but for different models...\n",
    "\n",
    "### Bart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|██████████| 191/191 [00:00<00:00, 34.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "bart_eng_model_name ='facebook/bart-base'\n",
    "bart_span_model_name = 'vgaraujov/bart-base-spanish'\n",
    "# Masking is a random process so results will vary unless this is set\n",
    "# set_seed(0)\n",
    "\n",
    "bart_span_model = BartForConditionalGeneration.from_pretrained(bart_span_model_name)\n",
    "bart_span_tokenizer = AutoTokenizer.from_pretrained(bart_span_model_name)\n",
    "\n",
    "bart_eng_model = BartForConditionalGeneration.from_pretrained(bart_eng_model_name)\n",
    "bart_eng_tokenizer = BartTokenizer.from_pretrained(bart_eng_model_name)\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_eng_model_name = 'roberta-base'\n",
    "roberta_span_model_name = 'bertin-project/bertin-roberta-base-spanish'\n",
    "\n",
    "roberta_eng_model = RobertaForMaskedLM.from_pretrained(roberta_eng_model_name)\n",
    "roberta_eng_tokenizer = RobertaTokenizer.from_pretrained(roberta_eng_model_name)\n",
    "\n",
    "\n",
    "roberta_span_model = RobertaForMaskedLM.from_pretrained(roberta_span_model_name)\n",
    "roberta_span_tokenizer = RobertaTokenizer.from_pretrained(roberta_span_model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an Evaluation Dataset class for Bart..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, input_ids, decoder_input_ids):\n",
    "        assert len(input_ids) == len(decoder_input_ids)\n",
    "        self.input_ids         = input_ids\n",
    "        self.decoder_input_ids = decoder_input_ids\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input_ids':         self.input_ids[index],\n",
    "                'decoder_input_ids': self.decoder_input_ids[index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as some methods to run our testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model=None, sentences=None, seq_len=None, tokenizer=None, num_test_chars=None, batch_size=8, mlm_prob=0.15, d_flag=False):\n",
    "        samples = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)['input_ids']\n",
    "        # Add bos and eos tokens and create the decoder_input_ids\n",
    "        # mask_token_id = 50264\n",
    "        bos = torch.LongTensor([tokenizer.bos_token_id])               # = 0\n",
    "        eos = torch.LongTensor([tokenizer.eos_token_id])               # = 2\n",
    "        if(model.config.decoder_start_token_id != None):\n",
    "            dst = torch.LongTensor([model.config.decoder_start_token_id]) # = 2 (same as eos token id)\n",
    "        else:\n",
    "              dst = bos\n",
    "        input_ids   = [torch.cat((torch.cat((bos, sample)), eos)) for sample in samples]\n",
    "        decoder_ids = [torch.cat((dst, input_id))[:-1] for input_id in input_ids]  # shift_tokens_right\n",
    "\n",
    "        # Put this all into a dataset and create the loader\n",
    "        # The collator will take care of randomly masking the input_id tokens and creating the \n",
    "        # 'labels' keys with -100 for any non-masked token\n",
    "        dataset    = EvalDataset(input_ids, decoder_ids)\n",
    "        collator   = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=mlm_prob)\n",
    "        dataloader = DataLoader(dataset, collate_fn=collator, batch_size=batch_size)\n",
    "\n",
    "        # Run evaluation\n",
    "        print('Testing')\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(tqdm(dataloader, ncols=100, disable=False)):\n",
    "            with torch.no_grad():\n",
    "                torch.set_printoptions(threshold=10000, linewidth=150)\n",
    "                decoder_ids = batch['decoder_input_ids']\n",
    "                input_ids   = batch['input_ids']\n",
    "                labels      = batch['labels']\n",
    "                if d_flag:\n",
    "                    outputs = model(input_ids=input_ids, labels=labels)\n",
    "                else:\n",
    "                    outputs = model(input_ids=input_ids, labels=labels, decoder_input_ids=decoder_ids)\n",
    "\n",
    "            losses.append(torch.exp(torch.FloatTensor([outputs.loss.item()])))\n",
    "        try:\n",
    "            perplexity = torch.mean(torch.FloatTensor(losses)).item()\n",
    "        except OverflowError:\n",
    "            perplexity = float('inf')\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 188/188 [08:44<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "bart_eng_ppl = run_test(bart_eng_model, eng_sentences, None, bart_eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/188 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████████████████████████| 188/188 [08:27<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "bart_span_ppl = run_test(bart_span_model, span_sentences, None, bart_span_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 188/188 [37:47<00:00, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 188/188 [44:21<00:00, 14.16s/it]\n"
     ]
    }
   ],
   "source": [
    "bart_es_en_ppl = run_test(bart_eng_model, flattened_es_en_sentences, None, bart_eng_tokenizer)\n",
    "bart_en_es_ppl = run_test(bart_span_model, flattened_en_es_sentences, None, bart_span_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.4083709716797\n",
      "421.5256042480469\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(torch.FloatTensor(bart_eng_ppl)).item())\n",
    "print(torch.mean(torch.FloatTensor(bart_span_ppl)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909.12158203125\n",
      "792.385986328125\n"
     ]
    }
   ],
   "source": [
    "print(bart_en_es_ppl)\n",
    "print(bart_es_en_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████▉                                                      | 24/188 [01:01<07:02,  2.58s/it]"
     ]
    }
   ],
   "source": [
    "roberta_eng_ppl = run_test(roberta_eng_model, eng_sentences, None, roberta_eng_tokenizer)\n",
    "roberta_span_ppl = run_test(roberta_span_model, span_sentences, None, roberta_span_tokenizer)\n",
    "roberta_es_en_ppl = run_test(roberta_eng_model, flattened_es_en_sentences, None, roberta_eng_tokenizer)\n",
    "roberta_en_es_ppl = run_test(roberta_span_model, flattened_en_es_sentences, None, roberta_span_tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
